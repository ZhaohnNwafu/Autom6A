#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
ä¼˜åŒ–ç‰ˆLLMä¿¡æ¯æå–å™¨
æ›´æ™ºèƒ½ã€æ›´å¿«ã€æ›´ç¨³å®š
"""

import json
import re,os
from openai import OpenAI

class LLMExtractor:
    """æ™ºèƒ½çš„ä¿¡æ¯æå–å™¨"""
    def __init__(self, api_key, model_name="deepseek-chat"):
        self.api_key = api_key
        self.model_name = model_name
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/"
        )
        # é¢„ç¼–è¯‘çš„ç³»ç»Ÿæç¤º(åªéœ€åŠ è½½ä¸€æ¬¡)
        self.system_prompt = self._load_system_prompt()
        print(f"[INFO] Using model: {model_name}")
    
    def _load_system_prompt(self) -> str:
        """åŠ è½½ç³»ç»Ÿæç¤º - æ–°å¢ä¸‰ä»£æµ‹åºçŸ¥è¯†"""
        return """ä½ æ˜¯ç”Ÿä¿¡AIåŠ©æ‰‹,ä»å¯¹è¯ä¸­æå–åˆ†ææ‰€éœ€ä¿¡æ¯ã€‚

æ ¸å¿ƒæ§½ä½(4ä¸ªå¿…éœ€):
1. data_type - æ•°æ®ç±»å‹(MeRIP-seq/RNA-seq/Nanopore-m6A...)
2. files - æ–‡ä»¶è·¯å¾„(æ”¯æŒç›®å½•/åˆ—è¡¨/é€šé…ç¬¦)
3. output_dir - è¾“å‡ºç›®å½•
4. goal - åˆ†æç›®æ ‡(ä¿ç•™ç”¨æˆ·åŸæ–‡)

ğŸ“ **è·¯å¾„è§£æè§„åˆ™ï¼ˆæå…¶é‡è¦ï¼‰**:
å½“ç”¨æˆ·è¯´"æ•°æ®è·¯å¾„ä¸ºAï¼ŒBï¼ŒC"æˆ–"æ–‡ä»¶æ˜¯A, B, C"æ—¶ï¼š
- è¯†åˆ«æ‰€æœ‰è·¯å¾„ï¼ŒæŒ‰æ¢è¡Œç¬¦åˆ†éš”
- doradoè·¯å¾„ï¼šåŒ…å«"dorado"å­—ç¬¦ä¸²çš„è·¯å¾„
- fast5ç›®å½•ï¼šåŒ…å«"fast5"çš„ç›®å½•è·¯å¾„
- å‚è€ƒåŸºå› ç»„ï¼š.fa, .fastaç»“å°¾çš„æ–‡ä»¶
- **å¿…é¡»å…¨éƒ¨æå–**ï¼Œä¸èƒ½åªæå–æœ€åä¸€ä¸ªï¼

ç¤ºä¾‹è¾“å…¥ï¼š"æ•°æ®è·¯å¾„ä¸º/tools/dorado/bin/doradoï¼Œ/data/fast5_dirï¼Œ/ref/genome.fa"
æ­£ç¡®è¾“å‡ºï¼š
```json
{
  "files": "/tools/dorado/bin/dorado: dorado basecaller executable\\n/data/fast5_dir: directory containing fast5 files\\n/ref/genome.fa: reference genome"
}
```

ğŸ”„ **ä¿®æ”¹æ„å›¾è¯†åˆ«ï¼ˆå…³é”®ï¼‰**:
ä¿®æ”¹å…³é”®è¯ï¼š["ä¿®æ”¹"ï¼Œ"æ”¹æˆ"ï¼Œ"æ”¹ä¸º"ï¼Œ"æ¢æˆ"ï¼Œ"åº”è¯¥æ˜¯"ï¼Œ"ä¸å¯¹"ï¼Œ"é‡æ–°"ï¼Œ"æ›´æ­£"ï¼Œ"æ›´æ–°"]

- å¦‚æœç”¨æˆ·è¯´"ä¿®æ”¹XXä¸ºYY" â†’ files_action: "replace"
- å¦‚æœç”¨æˆ·è¯´"è¾“å‡ºç›®å½•æ”¹æˆ/new/path" â†’ ç›´æ¥æ›¿æ¢output_dirï¼Œfiles_action: "replace"
- å¦‚æœç”¨æˆ·è¯´"æ·»åŠ /å¢åŠ " â†’ files_action: "add"
- **é»˜è®¤æƒ…å†µ**ï¼šæ–°æä¾›çš„å®Œæ•´è·¯å¾„åˆ—è¡¨ â†’ files_action: "replace"

ç¤ºä¾‹ï¼š
```
å·²æœ‰files: "/old/path"
ç”¨æˆ·: "æ”¹æˆ/new/path" â†’ {"files":"/new/path", "files_action":"replace"}
ç”¨æˆ·: "è¿˜æœ‰/add/path" â†’ {"files":"/add/path", "files_action":"add"}
```
ğŸ†• ä¸‰ä»£æµ‹åºä¸“ç”¨è¯†åˆ«:
- å…³é”®è¯: ä¸‰ä»£/çº³ç±³å­”/Nanopore/ONT/Oxford/fast5/MinION/PromethION
- æ•°æ®ç±»å‹è‡ªåŠ¨æ˜ å°„ä¸º: Nanopore-m6A / Nanopore-RNA / Nanopore
- æ–‡ä»¶æ ¼å¼: fast5 (åŸå§‹ä¿¡å·) æˆ– fastq (basecalled)

å¯é€‰æ§½ä½:
5. species (Arabidopsis/Rice/Human/Mouse)
6. sample_count, threads...
7. files_action (add/replace/auto)

å…³é”®è§„åˆ™:
â€¢ goalå¿…é¡»å®Œæ•´ä¿ç•™ç”¨æˆ·åŸè¯,ä¸åšä»»ä½•ç®€åŒ–æˆ–ç¿»è¯‘
â€¢ æ™ºèƒ½æ¨æ–­: "m6A"â†’MeRIP-seq, "æ‹Ÿå—èŠ¥"â†’Arabidopsis, "ä¸‰ä»£m6A"â†’Nanopore-m6A, "fast5æ–‡ä»¶"â†’Nanopore
â€¢ **è·¯å¾„å¿…é¡»å…¨éƒ¨æå–ï¼Œç”¨æ¢è¡Œç¬¦åˆ†éš”**
â€¢ **å‡†ç¡®è¯†åˆ«ä¿®æ”¹vså¢åŠ æ„å›¾**
â€¢ è¿”å›JSON: {"extracted_info": {...}, "missing_slots": [...], "next_question": "..."}"""

    def extract(self, user_input, current_slots, conversation_history=None):
        """ä»ç”¨æˆ·è¾“å…¥ä¸­æå–ä¿¡æ¯ - è¶…çº§æ™ºèƒ½ç‰ˆæœ¬"""
        # 1. å¿«é€Ÿæ­£åˆ™é¢„æå–(1mså†…å®Œæˆ)
        direct_result = self._smart_fallback_extract(user_input, current_slots)
        
        # 2. å¦‚æœæ­£åˆ™æå–åˆ°å…³é”®ä¿¡æ¯,ä¸”ç½®ä¿¡åº¦é«˜,ç›´æ¥è¿”å›
        if self._is_high_confidence_extraction(direct_result):
            print(f"[INFO] Using fast regex extraction (confident)")
            return direct_result
        
        # 3. å¦åˆ™è°ƒç”¨LLM(å¸¦ç¼“å­˜)
        try:
            prompt = self._build_super_prompt(user_input, current_slots, conversation_history)
            response = self._call_llm(prompt)
            result = self._parse_response(response, current_slots)
            
            # 4. åˆå¹¶LLMå’Œæ­£åˆ™çš„ç»“æœ(å–æœ€ä¼˜)
            return self._merge_results(result, direct_result)
            
        except Exception as e:
            print(f"[ERROR] LLM extraction failed: {e}")
            # é™çº§åˆ°æ­£åˆ™ç»“æœ
            return direct_result
    
    def _build_super_prompt(self, user_input, current_slots, history):
        """æ„å»ºè¶…çº§æ™ºèƒ½æç¤ºè¯"""   
        # å½“å‰çŠ¶æ€(ç´§å‡‘æ ¼å¼)
        filled = current_slots.get_filled_slots()
        missing = current_slots.get_missing_required_slots()
        
        # ç‰¹åˆ«æ ‡æ³¨å·²æœ‰files
        files_hint = ""
        if 'files' in filled:
            file_lines = filled['files'].split('\n')
            count = len(file_lines)
            files_hint = f"\nâš ï¸ å·²æœ‰{count}ä¸ªæ–‡ä»¶/è·¯å¾„:\n"
            for i, line in enumerate(file_lines[:3], 1):
                files_hint += f"  {i}. {line[:60]}...\n" if len(line) > 60 else f"  {i}. {line}\n"
            if count > 3:
                files_hint += f"  ... å…±{count}é¡¹"

        state_str = f"""ğŸ“Š å½“å‰: {json.dumps(filled, ensure_ascii=False) if filled else 'ç©º'}{files_hint}
â“ ç¼ºå¤±: {', '.join(missing) if missing else 'å®Œæ•´'} ({current_slots.get_completeness_percentage()}%)"""
        
        # ç²¾ç®€å†å²(åªä¿ç•™æœ€è¿‘2è½®æ‘˜è¦)
        history_str = ""
        if history and len(history) > 0:
            recent = history[-2:]
            history_str = "\nğŸ’¬ å†å²:\n" + "\n".join([
                f"ç”¨æˆ·: {msg[0][:60]}..." if len(msg[0]) > 60 else f"ç”¨æˆ·: {msg[0]}"
                for msg in recent if isinstance(msg, list) and len(msg) >= 1
            ])
        
        # æ ¸å¿ƒç¤ºä¾‹(åªä¿ç•™æœ€å…³é”®çš„3ä¸ª)
        examples = """å…³é”®ç¤ºä¾‹:
1ï¸âƒ£ **å¤šè·¯å¾„è§£æ**ï¼ˆæœ€é‡è¦ï¼‰:
è¾“å…¥: "æ•°æ®è·¯å¾„ä¸º/tools/dorado/bin/doradoï¼Œ/data/fast5ï¼Œ/ref/genome.fa"
è¾“å‡º: {"files":"/tools/dorado/bin/dorado: dorado basecaller\\n/data/fast5: fast5 directory\\n/ref/genome.fa: reference genome"}

2ï¸âƒ£ **ä¿®æ”¹vså¢åŠ **:
å·²æœ‰files: "/old/path"
è¾“å…¥: "æ”¹æˆ/new/path" â†’ {"files":"/new/path","files_action":"replace"}
è¾“å…¥: "è¿˜æœ‰/add/path" â†’ {"files":"/add/path","files_action":"add"}

3ï¸âƒ£ **å®Œæ•´è·¯å¾„åˆ—è¡¨**ï¼ˆNanoporeï¼‰:
è¾“å…¥: "è·¯å¾„ä¸ºAï¼ŒBï¼ŒC"ï¼ˆ3ä¸ªè·¯å¾„ï¼‰
è¾“å‡º: {"files":"A\\nB\\nC","files_action":"replace"}"""
        
        # ç”¨æˆ·æç¤º
        user_prompt = f"""{state_str}
{history_str}

{examples}

ğŸ¯ å½“å‰è¾“å…¥: "{user_input}"

**ä»»åŠ¡**:
1. æå–æ‰€æœ‰è·¯å¾„ï¼ˆé€—å·åˆ†éš”çš„å¤šä¸ªè·¯å¾„å¿…é¡»å…¨éƒ¨æå–ï¼‰
2. åˆ¤æ–­æ˜¯ä¿®æ”¹(replace)è¿˜æ˜¯å¢åŠ (add)
3. goalä¿ç•™åŸæ–‡
4. è¿”å›JSON

æ³¨æ„ï¼šå¦‚æœç”¨æˆ·æä¾›å¤šä¸ªè·¯å¾„ï¼ˆå¦‚"Aï¼ŒBï¼ŒC"ï¼‰ï¼Œå¿…é¡»å…¨éƒ¨æå–å¹¶ç”¨\\nåˆ†éš”ï¼"""
        
        return self.system_prompt, user_prompt

    def _is_high_confidence_extraction(self, result) -> bool:
        """åˆ¤æ–­æ­£åˆ™æå–æ˜¯å¦é«˜ç½®ä¿¡åº¦"""
        extracted = result.get('extracted_info', {})
        # å¦‚æœæå–åˆ°3ä¸ªä»¥ä¸Šå…³é”®å­—æ®µ,è®¤ä¸ºæ˜¯é«˜ç½®ä¿¡åº¦
        key_fields = {'data_type', 'files', 'output_dir', 'goal', 'species'}
        extracted_keys = set(extracted.keys()) & key_fields
        return len(extracted_keys) >= 3
    
    def _merge_results(self, llm_result, regex_result):
        """åˆå¹¶LLMå’Œæ­£åˆ™çš„æå–ç»“æœ,å–æœ€ä¼˜"""
        merged_info = {}
        
        # LLMä¼˜å…ˆ(é€šå¸¸æ›´å‡†ç¡®)
        merged_info.update(llm_result.get('extracted_info', {}))
        
        # æ­£åˆ™è¡¥å……LLMé—æ¼çš„
        for key, value in regex_result.get('extracted_info', {}).items():
            if key not in merged_info or not merged_info[key]:
                merged_info[key] = value
        
        return {
            'extracted_info': merged_info,
            'missing_slots': llm_result.get('missing_slots', []),
            'next_question': llm_result.get('next_question', ''),
            'confidence': max(llm_result.get('confidence', 0), regex_result.get('confidence', 0))
        }
    
    def _call_llm(self, prompt):
        """è°ƒç”¨DeepSeek API - ä¼˜åŒ–ç‰ˆ"""
        system_prompt, user_prompt = prompt
        max_retries = 3
        retry_count = 0
        while retry_count < max_retries:
            try:
                print(f"[DEBUG] Calling LLM API (attempt {retry_count + 1}/{max_retries})...")
                
                # ä¸ä½¿ç”¨ response_formatï¼Œå› ä¸ºå¯èƒ½å¯¼è‡´ç©ºå“åº”
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.2,
                    max_tokens=1000
                )
                
                content = response.choices[0].message.content
                
                # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                if not content or not content.strip():
                    print(f"[WARN] Empty response from LLM")
                    if retry_count < max_retries - 1:
                        retry_count += 1
                        continue
                    else:
                        raise Exception("LLMè¿”å›ç©ºå“åº”")
                
                print(f"[DEBUG] LLM API call successful, response length: {len(content)}")
                return content
                
            except Exception as e:
                retry_count += 1
                print(f"[ERROR] API call failed (attempt {retry_count}/{max_retries}): {error_msg}")
                
                if retry_count < max_retries:
                    import time
                    time.sleep(retry_count*2)
                else:
                    raise Exception(f"APIè°ƒç”¨å¤±è´¥: {e}")
    
    def _parse_response(self, response_text, current_slots):
        """è§£æLLMå“åº” - å¢å¼ºå®¹é”™"""
        print(f"[DEBUG] Raw LLM response (first 500 chars): {response_text[:500]}")
        
        try:
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            cleaned = response_text.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()
            
            # è§£æJSON
            result = json.loads(cleaned)
            
            # éªŒè¯å’Œä¿®å¤
            if 'extracted_info' not in result:
                result['extracted_info'] = {}

            # é‡è¦ï¼šç¡®ä¿extracted_infoä¸­çš„å€¼ä¸æ˜¯None
            if result['extracted_info']:
                result['extracted_info'] = {k: v for k, v in result['extracted_info'].items() if v is not None}  
            
            if 'missing_slots' not in result:
                # è‡ªåŠ¨è®¡ç®—ç¼ºå¤±æ§½ä½
                updated_slots = current_slots.get_all_slots().copy()
                if result['extracted_info']:
                    updated_slots.update(result['extracted_info'])
                missing = [k for k, v in updated_slots.items() 
                          if v is None and k in current_slots.required_slots]
                result['missing_slots'] = missing
            
            if 'next_question' not in result or not result['next_question']:
                # ç”Ÿæˆé»˜è®¤é—®é¢˜
                missing = result.get('missing_slots', [])
                if missing:
                    result['next_question'] = f"è¿˜éœ€è¦ï¼š{', '.join(missing)}"
                else:
                    result['next_question'] = "ä¿¡æ¯å·²å®Œæ•´ï¼"
            
            result.setdefault('confidence', 0.8)
            result.setdefault('reasoning', '')
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"[DEBUG] Extracted: {result['extracted_info']}")
            print(f"[DEBUG] Missing: {result['missing_slots']}")
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"[ERROR] JSON parse failed: {e}")
            return self._smart_fallback_extract(response_text, current_slots)
    
    def _smart_fallback_extract(self, text, current_slots):
        """å¤‡ç”¨æå–æ–¹æ¡ˆ - å¢å¼ºä¸‰ä»£æ•°æ®è¯†åˆ«"""
        print("[INFO] Using fallback extraction")
        
        # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²
        if not isinstance(text, str):
            text = str(text)

        extracted = {}
        text_lower = text.lower()

        # ============ ğŸ†• ä¼˜å…ˆæ£€æµ‹ä¸‰ä»£æµ‹åºæ•°æ® ============
        nanopore_keywords = [
            'nanopore', 'çº³ç±³å­”', 'ä¸‰ä»£', 'ont', 'oxford',
            'minion', 'promethion', 'fast5', 'gridion'
        ]
        
        is_nanopore = any(kw in text_lower for kw in nanopore_keywords)
        
        if is_nanopore:
            extracted['data_type'] = 'Nanopore-m6A'
            extracted['sequencing_platform'] = 'Nanopore'
            print(f"[FALLBACK] âœ“ Detected Nanopore sequencing data")

        # ============ æå–æ–‡ä»¶è·¯å¾„ ============
        files_info = self._extract_files_smartly(text, text_lower)
        if files_info:
            extracted.update(files_info)
 
        # ============ æå–åˆ†æç›®æ ‡  ============
        # å°è¯•è¯†åˆ«åŒ…å«ç›®æ ‡æè¿°çš„éƒ¨åˆ†
        goal_patterns = [
            r'(?:åˆ†æç›®æ ‡|ç›®æ ‡|goal|analysis|è¦åš|éœ€è¦|æƒ³è¦)[ï¼š:]\s*(.+?)(?:\n|$)',
            r'(?:ç”¨|ä½¿ç”¨|with|using)\s+\w+\s*(?:åš|è¿›è¡Œ|æ¥|for)',  # åŒ¹é…å«å·¥å…·åçš„æè¿°
        ]
        
        goal_text = None
        for pattern in goal_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                goal_text = match.group(1) if match.lastindex else match.group(0)
                break
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ç›®æ ‡æ ‡è®°ï¼Œæ£€æŸ¥æ˜¯å¦æ•´æ®µéƒ½æ˜¯æè¿°ç›®æ ‡
        if not goal_text and any(kw in text_lower for kw in ['è´¨æ§', 'qc', 'æ¯”å¯¹', 'align', 'peak', 'åˆ†æ', 'analysis', 'basecalling','m6anet','nanopolish']):
            # æå–åŒ…å«åˆ†æç›¸å…³å…³é”®è¯çš„å†…å®¹
            sentences = [s.strip() for s in text.split('ï¼Œ') if s.strip()]
            goal_sentences = [s for s in sentences if any(kw in s.lower() for kw in 
                ['è´¨æ§', 'qc', 'fastqc', 'æ¯”å¯¹', 'align', 'hisat', 'peak', 'macs', 'åˆ†æ','basecalling','dorado','m6anet','nanopolish'])]
            if goal_sentences:
                goal_text = ', '.join(goal_sentences)
        
        if goal_text:
            extracted['goal'] = goal_text.strip()
            print(f"[FALLBACK] Extracted goal (original): {extracted['goal'][:100]}...")
        
        # ================ æå–ç‰©ç§===================
        species_patterns = {
            'Arabidopsis': [r'æ‹Ÿå—èŠ¥', r'arabidopsis', r'\bath\b', r'tair'],
            'Rice': [r'æ°´ç¨»', r'rice', r'oryza'],
            'Human': [r'äººç±»?', r'human', r'homo\s+sapiens'],
            'Mouse': [r'å°é¼ ', r'mouse', r'mus\s+musculus']
        }
        for species, patterns in species_patterns.items():
            for pattern in patterns:
                try:
                    if re.search(pattern, text_lower):
                        extracted['species'] = species
                        print(f"[FALLBACK] Extracted species: {species}")
                        break
                except Exception as e:
                    print(f"[WARN] Regex error for pattern {pattern}: {e}")
            if 'species' in extracted:
                break
        
        # ============ æå–æ•°æ®ç±»å‹ ============
        if 'data_type' not in extracted:
            datatype_patterns = {
                'MeRIP-seq': [r'm6a', r'merip', r'ç”²åŸºåŒ–', r'm6A'],
                'RNA-seq': [r'rna[-\s]?seq', r'è½¬å½•ç»„', r'rnaæµ‹åº'],
                'ChIP-seq': [r'chip[-\s]?seq', r'èŠ¯ç‰‡æµ‹åº']
            }
            for dtype, patterns in datatype_patterns.items():
                for pattern in patterns:
                    try:
                        if re.search(pattern, text_lower):
                            extracted['data_type'] = dtype
                            print(f"[FALLBACK] Extracted data_type: {dtype}")
                            break
                    except: pass         
                if 'data_type' in extracted:
                    break
            
        # ============ è®¡ç®—ç¼ºå¤±æ§½ä½ ============
        all_slots = current_slots.required_slots.copy()
        all_slots.update(extracted)
        missing = [k for k, v in all_slots.items() if v is None]
        
        # ç”Ÿæˆæ›´å‹å¥½çš„é—®é¢˜
        if missing:
            missing_cn = {
                'species': 'ç‰©ç§','data_type': 'æ•°æ®ç±»å‹',
                'files': 'æ–‡ä»¶è·¯å¾„','output_dir': 'è¾“å‡ºç›®å½•',
                'goal': 'åˆ†æç›®æ ‡'
            }
            missing_names = [missing_cn.get(m, m) for m in missing]
            # ğŸ†• ä¸‰ä»£æ•°æ®ç‰¹æ®Šæç¤º
            if is_nanopore and 'files' in missing:
                next_question = f"æˆ‘ç†è§£äº†è¿™æ˜¯Nanoporeä¸‰ä»£æµ‹åºæ•°æ®ã€‚è¿˜éœ€è¦ï¼š{', '.join(missing_names)}ã€‚\nâš ï¸ è¯·æä¾›doradoè·¯å¾„ã€fast5ç›®å½•å’Œå‚è€ƒåŸºå› ç»„/è½¬å½•ç»„"
            else:
                next_question = f"æˆ‘ç†è§£äº†éƒ¨åˆ†ä¿¡æ¯ã€‚è¿˜éœ€è¦ï¼š{', '.join(missing_names)}"
        else:
            next_question = "ä¿¡æ¯å·²å®Œæ•´ï¼"
        
        result = {
            'extracted_info': extracted,
            'missing_slots': missing,
            'next_question': next_question,
            'confidence':0.85 if is_nanopore else 0.7,
            'reasoning': 'æ£€æµ‹åˆ°Nanoporeæ•°æ®' if is_nanopore else 'ä½¿ç”¨æ™ºèƒ½è§„åˆ™æå–'
        }
        
        print(f"[FALLBACK] Final extracted: {extracted}")
        return result
    
    def _extract_files_smartly(self, text, text_lower):
        """æ™ºèƒ½æå–æ–‡ä»¶ä¿¡æ¯ - å¢å¼ºfast5è¯†åˆ«"""
        result = {}
        paths = []
        print(f"\n[FILES] Starting smart extraction...")
        
        # ============ ç­–ç•¥1: ä¼˜å…ˆå¤„ç†é€—å·åˆ†éš”çš„å¤šè·¯å¾„ ============
        multi_path_patterns = [
            r'(?:è·¯å¾„|æ–‡ä»¶|æ•°æ®)(?:ä¸º|æ˜¯|ï¼š|:)\s*(.+?)(?:\n|$|ã€‚)',
            r'(?:æœ‰|åŒ…æ‹¬|åŒ…å«)(?:æ–‡ä»¶|æ•°æ®)?\s*[:ï¼š]?\s*(.+?)(?:\n|$|ã€‚)',
        ]
        
        multi_paths_found = False
        for pattern in multi_path_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                path_str = match.group(1).strip()
                print(f"[FILES] Step 1: Matched path string: {path_str[:100]}")
                
                # æŒ‰æ ‡ç‚¹åˆ†å‰²
                raw_paths = re.split(r'[,ï¼Œã€;ï¼›]', path_str)
                print(f"[FILES] Step 2: Split into {len(raw_paths)} parts")
                
                for i, p in enumerate(raw_paths, 1):
                    p = p.strip()
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆè·¯å¾„
                    if (p.startswith('/') or re.match(r'[A-Z]:\\', p)) and len(p) > 3:
                        paths.append(p)
                        print(f"[FILES]   Part {i}: âœ“ Valid path: {p[:60]}")
                        multi_paths_found = True
                    else:
                        print(f"[FILES]   Part {i}: âœ— Not a path: {p[:60]}")
                
                if multi_paths_found:
                    break
        
        # ============ ç­–ç•¥2: å¤‡ç”¨å•è·¯å¾„æå– ============
        if not multi_paths_found:
            print("[FILES] Step 1: No multi-path pattern, using single extraction")
            try:
                unix_paths = re.findall(r'/[^\sï¼Œ,;ï¼›ã€‚!ï¼?ï¼Ÿ\n]+', text)
                paths.extend(unix_paths)
                print(f"[FILES] Step 2: Found {len(unix_paths)} Unix paths")
            except Exception as e:
                print(f"[WARN] Unix path extraction error: {e}")
            
            try:
                win_paths = re.findall(r'[A-Z]:\\[^\sï¼Œ,;ï¼›ã€‚!ï¼?ï¼Ÿ\n]+', text)
                paths.extend(win_paths)
            except: pass
        
        print(f"[FILES] Total found: {len(paths)} paths")

        # ============ ç­–ç•¥3: æ™ºèƒ½åˆ†ç±»è·¯å¾„ ============
        bio_exts = ['.fastq', '.fq', '.fastq.gz', '.fq.gz', '.bam', '.sam', 
                    '.fasta', '.fa', '.fast5', 'pod5', '.bed', '.vcf', '.gtf', '.gff']
        
        final_files = []
        dorado_path = None
        fast5_dir = None
        reference_genome = None
        output_dir = None
        
        print(f"[FILES] Step 3: Classifying paths...")
        
        for path in paths:
            try:
                path = path.rstrip('ã€‚ï¼Œ,;ï¼›!ï¼?ï¼Ÿã€')
                if not path or len(path) < 2:
                    continue
                
                path_lower = path.lower()
                
                # ä¼˜å…ˆè¯†åˆ«dorado
                if 'dorado' in path_lower and 'bin' in path_lower:
                    dorado_path = path
                    print(f"[FILES]   âœ“ Dorado executable: {path}")
                    continue
                
                # è¯†åˆ«fast5ç›®å½•
                if 'fast5' in path_lower and not path.endswith('.fast5'):
                    fast5_dir = path
                    print(f"[FILES]   âœ“ Fast5 directory: {path}")
                    continue
                
                # è¯†åˆ«å‚è€ƒåŸºå› ç»„
                if path.endswith(('.fa', '.fasta', '.fa.gz', '.fasta.gz')):
                    reference_genome = path
                    print(f"[FILES]   âœ“ Reference genome: {path}")
                    continue
                
                # ç”Ÿä¿¡æ–‡ä»¶
                is_bio_file = any(path_lower.endswith(ext) for ext in bio_exts)
                if is_bio_file:
                    final_files.append(path)
                    print(f"[FILES]   âœ“ Bio file: {os.path.basename(path)}")
                    continue
                
                # ä¸Šä¸‹æ–‡åˆ†æ
                path_idx = text.find(path)
                if path_idx >= 0:
                    context_start = max(0, path_idx - 30)
                    context_end = min(len(text), path_idx + len(path) + 30)
                    context = text[context_start:context_end].lower()
                    
                    output_keywords = ['è¾“å‡º', 'output', 'ç»“æœ', 'ä¿å­˜']
                    input_keywords = ['æ•°æ®', 'data', 'æ–‡ä»¶', 'file', 'ç›®å½•']
                    
                    if any(kw in context for kw in output_keywords):
                        output_dir = path
                        print(f"[FILES]   âœ“ Output dir: {path}")
                    elif any(kw in context for kw in input_keywords):
                        final_files.append(path)
                        print(f"[FILES]   âœ“ Input dir: {path}")
                
            except Exception as e:
                print(f"[WARN] Error processing path '{path}': {e}")

        # ============ ç­–ç•¥4: ç»„è£…fileså­—æ®µï¼ˆNanoporeä¼˜å…ˆï¼‰ ============
        assembled_files = []
        
        if dorado_path:
            assembled_files.append(f"{dorado_path}: dorado basecaller executable")
        
        if fast5_dir:
            assembled_files.append(f"{fast5_dir}: directory containing fast5 files")
        
        if reference_genome:
            assembled_files.append(f"{reference_genome}: reference genome")
        
        # æ·»åŠ å…¶ä»–æ–‡ä»¶
        for f in final_files:
            if f not in [dorado_path, fast5_dir, reference_genome]:
                assembled_files.append(f)
        
        if assembled_files:
            result['files'] = '\n'.join(assembled_files)
            print(f"[FILES] âœ… Assembled {len(assembled_files)} files:")
            for i, f in enumerate(assembled_files, 1):
                print(f"[FILES]   {i}. {f[:80]}")
        
        if output_dir:
            result['output_dir'] = output_dir
            print(f"[FILES] âœ… Output directory: {output_dir}")
        
        # ============ ç­–ç•¥5: æ£€æµ‹ä¿®æ”¹æ„å›¾ ============
        modify_keywords = ['ä¿®æ”¹', 'æ”¹æˆ', 'æ”¹ä¸º', 'æ¢æˆ', 'åº”è¯¥æ˜¯', 'ä¸å¯¹', 'é‡æ–°', 'æ›´æ­£']
        add_keywords = ['æ·»åŠ ', 'å¢åŠ ', 'è¿˜æœ‰', 'ä»¥åŠ', 'å¦å¤–']
        
        has_modify = any(kw in text for kw in modify_keywords)
        has_add = any(kw in text for kw in add_keywords)
        
        if has_modify:
            result['files_action'] = 'replace'
            print(f"[FILES] ğŸ”„ Intent: REPLACE (modification keywords)")
        elif has_add:
            result['files_action'] = 'add'
            print(f"[FILES] â• Intent: ADD (addition keywords)")
        else:
            if len(assembled_files) >= 2:
                result['files_action'] = 'replace'
                print(f"[FILES] ğŸ”„ Intent: REPLACE (multiple paths)")
            else:
                result['files_action'] = 'auto'
                print(f"[FILES] ğŸ¤– Intent: AUTO")
        
        print(f"[FILES] Extraction complete\n")
        
        return result if result else None